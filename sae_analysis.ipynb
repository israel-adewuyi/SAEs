{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b825c978-6201-466b-8b3d-5b53357a7899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install transformer-lens jaxtyping datasets sae-lens circuitsvis\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2255718-e0db-46c2-8198-70d3c26ab4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import einops\n",
    "import random\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "from torch import Tensor\n",
    "from rich.table import Table\n",
    "from jaxtyping import Float, Int\n",
    "from rich import print as rprint\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "from sae_lens import SAE, load_model, ActivationsStore, HookedSAETransformer\n",
    "from datasets import load_dataset\n",
    "from transformer_lens import HookedTransformer\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "from sae_lens import LanguageModelSAERunnerConfig, CacheActivationsRunnerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9264965-f484-4c98-a6a9-ff8e107d6614",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3ee88d-ecbf-4df0-b452-f7b89d18d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def highest_activating_tokens(\n",
    "    tokens: Int[Tensor, \"batch seq\"],\n",
    "    model: HookedTransformer,\n",
    "    autoencoder: SAE,\n",
    "    feature_idx: int,\n",
    "    autoencoder_B: bool = False,\n",
    "    k: int = 20,\n",
    ") -> Tuple[Int[Tensor, \"k 2\"], Float[Tensor, \"k\"]]:\n",
    "    '''\n",
    "    Returns the indices & values for the highest-activating tokens in the given batch of data.\n",
    "    '''\n",
    "    batch_size, seq_len = tokens.shape\n",
    "    # instance_idx = 1 if autoencoder_B else 0/\n",
    "\n",
    "    # Get the post activations from the clean run\n",
    "    cache = model.run_with_cache(tokens, names_filter=[\"blocks.8.hook_resid_post\"])[1]\n",
    "    post = cache[\"blocks.8.hook_resid_post\"]\n",
    "    post_reshaped = einops.rearrange(post, \"batch seq d_model -> (batch seq) d_model\")\n",
    "\n",
    "    del cache\n",
    "    # print(f\"Shape of tokens is {tokens.shape}\")\n",
    "    # print(f\"Shape of post is {post.shape}\")\n",
    "\n",
    "    # Compute activations (not from a fwd pass, but explicitly, by taking only the feature we want)\n",
    "    # This code is copied from the first part of the 'forward' method of the AutoEncoder class\n",
    "    h_cent = post_reshaped - autoencoder.b_dec\n",
    "    acts = einops.einsum(\n",
    "        h_cent, autoencoder.W_enc[:, feature_idx],\n",
    "        \"batch_size n_input_ae, n_input_ae -> batch_size\"\n",
    "    )\n",
    "    print(f\"Feature index is {feature_idx}\")\n",
    "\n",
    "    # Get the top k largest activations\n",
    "    top_acts_values, top_acts_indices = acts.topk(k)\n",
    "\n",
    "    del acts, post_reshaped, h_cent\n",
    "\n",
    "    # Convert the indices into (batch, seq) indices\n",
    "    top_acts_batch = top_acts_indices // seq_len\n",
    "    top_acts_seq = top_acts_indices % seq_len\n",
    "\n",
    "    return torch.stack([top_acts_batch, top_acts_seq], dim=-1), top_acts_values\n",
    "\n",
    "\n",
    "def display_top_sequences(top_acts_indices, top_acts_values, tokens):\n",
    "    table = Table(\"Sequence\", \"Activation\", title=\"Tokens which most activate this feature\")\n",
    "    for (batch_idx, seq_idx), value in zip(top_acts_indices, top_acts_values):\n",
    "        # Get the sequence as a string (with some padding on either side of our sequence)\n",
    "        seq = \"\"\n",
    "        for i in range(max(seq_idx-5, 0), min(seq_idx+5, tokens.shape[1])):\n",
    "            new_str_token = model.to_single_str_token(tokens[batch_idx, i].item()).replace(\"\\n\", \"\\\\n\")\n",
    "            # Highlight the token with the high activation\n",
    "            if i == seq_idx: new_str_token = f\"[b u dark_orange]{new_str_token}[/]\"\n",
    "            seq += new_str_token\n",
    "        # Print the sequence, and the activation value\n",
    "        table.add_row(seq, f'{value:.2f}')\n",
    "    rprint(table)\n",
    "\n",
    "def steering_hook(\n",
    "    activations: Float[Tensor, \"batch pos d_in\"],\n",
    "    hook: HookPoint,\n",
    "    sae: SAE,\n",
    "    latent_idx: int,\n",
    "    steering_coefficient: float,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Steers the model by returning a modified activations tensor, with some multiple of the steering vector added to all\n",
    "    sequence positions.\n",
    "    \"\"\"\n",
    "    return activations + steering_coefficient * sae.W_dec[latent_idx]\n",
    "\n",
    "\n",
    "GENERATE_KWARGS = dict(temperature=0.5, freq_penalty=2.0, verbose=False)\n",
    "\n",
    "\n",
    "def generate_with_steering(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    prompt: str,\n",
    "    latent_idx: int,\n",
    "    steering_coefficient: float = 1.0,\n",
    "    max_new_tokens: int = 50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates text with steering. A multiple of the steering vector (the decoder weight for this latent) is added to\n",
    "    the last sequence position before every forward pass.\n",
    "    \"\"\"\n",
    "    _steering_hook = partial(\n",
    "        steering_hook,\n",
    "        sae=sae,\n",
    "        latent_idx=latent_idx,\n",
    "        steering_coefficient=steering_coefficient,\n",
    "    )\n",
    "\n",
    "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, _steering_hook)]):\n",
    "        output = model.generate(prompt, max_new_tokens=max_new_tokens, **GENERATE_KWARGS)\n",
    "\n",
    "    return output\n",
    "\n",
    "def find_max_activation(\n",
    "    model: HookedTransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    feature_idx: int,\n",
    "    num_batches: int = 5,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Find the maximum activation for a given feature index, by iterating through\n",
    "    batches in the activation store and taking max over all of them. This is\n",
    "    useful for calibrating the right amount of the feature to add.\n",
    "\n",
    "    \"\"\"\n",
    "    max_act = 0.0\n",
    "\n",
    "    pbar = tqdm(range(num_batches))\n",
    "    for _ in pbar:\n",
    "        toks = actstore.get_batch_tokens().to('cuda:2')\n",
    "\n",
    "        cache = model.run_with_cache(toks, names_filter=[\"blocks.8.hook_resid_post\"], stop_at_layer=9)[1]\n",
    "        post = cache[\"blocks.8.hook_resid_post\"]\n",
    "        post_reshaped = einops.rearrange(post, \"batch seq d_model -> (batch seq) d_model\")\n",
    "\n",
    "\n",
    "        h_cent = post_reshaped - sae.b_dec\n",
    "        acts = einops.einsum(\n",
    "            h_cent, sae.W_enc[:, feature_idx],\n",
    "            \"batch_size n_input_ae, n_input_ae-> batch_size\"\n",
    "        )\n",
    "\n",
    "        act, _ = acts.topk(1)\n",
    "        max_act = max(act, max_act)\n",
    "        del cache, acts, toks\n",
    "    return max_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e7c57c-ded7-40a3-a35e-9891ad76bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_training_steps = 30_000  # probably we should do more\n",
    "batch_size = 4096\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = 0\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "l1_warm_up_steps = total_training_steps // 20  # 5% of training\n",
    "\n",
    "cfg = CacheActivationsRunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name= \"gpt2-small\",\n",
    "    model_class_name = \"HookedTransformer\",\n",
    "    hook_name= \"blocks.0.hook_resid_post\",\n",
    "    hook_layer=8,\n",
    "    dataset_path=\"apollo-research/Skylion007-openwebtext-tokenizer-gpt2\",\n",
    "    # dataset_trust_remote_code: bool | None = None\n",
    "    streaming = True,\n",
    "    is_dataset_tokenized = True,\n",
    "    context_size = 1024,\n",
    "    new_cached_activations_path = (\n",
    "        None  # Defaults to \"activations/{dataset}/{model}/{full_hook_name}_{hook_head_index}\"\n",
    "    ),\n",
    "    # dont' specify this since you don't want to load from disk with the cache runner.\n",
    "    cached_activations_path = None,\n",
    "    # SAE Parameters\n",
    "    d_in = 768,\n",
    "\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 20,\n",
    "    training_tokens = 2_000_000,\n",
    "    store_batch_size_prompts = 32,\n",
    "    train_batch_size_tokens = 4096,\n",
    "    normalize_activations = \"none\",  # should always be none for activation caching\n",
    "\n",
    "    # Misc\n",
    "    device = \"cuda:2\",\n",
    "    act_store_device = \"with_model\",  # will be set by post init if with_model\n",
    "    seed = 42,\n",
    "    dtype = \"float32\",\n",
    "    prepend_bos = True,\n",
    "    autocast_lm = False # autocast lm during activation fetching\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1605d5-9ae5-4b45-aff8-ee2e891a5f64",
   "metadata": {},
   "source": [
    "# GPT2-Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d10918-c0c1-4ab4-8c43-7739c5d0cf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/israel/mechintep/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda:3\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained('gpt2-small').to('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "825ab13d-7f30-4033-a5a9-08e977b279aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29af10fb8de462bbe6da947d38a44e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c1d2d562ac4ea19fa2585aa04688ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cfg = sae.cfg\n",
    "actstore = ActivationsStore.from_config(cfg=cfg,model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcbb5e33-ba74-47a6-9909-90503a4ffa08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = actstore.get_batch_tokens().to('cuda:2')\n",
    "\n",
    "all_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83ec80c7-37c2-4a27-a6c1-92b000bf6132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAE(\n",
       "  (activation_fn): ReLU()\n",
       "  (hook_sae_input): HookPoint()\n",
       "  (hook_sae_acts_pre): HookPoint()\n",
       "  (hook_sae_acts_post): HookPoint()\n",
       "  (hook_sae_output): HookPoint()\n",
       "  (hook_sae_recons): HookPoint()\n",
       "  (hook_sae_error): HookPoint()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\"israel-adewuyi/GPT2_small_sae\", \"resid_post/layer_8/width_25K/blocks.8.hook_resid_post\")\n",
    "sae.to('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92615ebe-80cd-4ae4-a57d-9bacc3b0e852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature index is 12608\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                Tokens which most activate this feature                 </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Sequence                                                </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│  mattress tied to her back<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">.\"</span> No charges were filed      │ 1.07       │\n",
       "│  some other type of handwriting<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">.</span> Cursive proponents say │ 1.00       │\n",
       "│  minimum payment on their accounts<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">.</span> It��s               │ 0.87       │\n",
       "│  a better grade.\\n<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">\\n</span>But the most efficient              │ 0.77       │\n",
       "│  and Pcdh17<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">.</span> Genes involved in                          │ 0.70       │\n",
       "│  normally only found among Arabs<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">.</span>&lt;|endoftext|&gt;If you�   │ 0.69       │\n",
       "│  many hundreds of years before<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">.</span> Huge trade was carried  │ 0.66       │\n",
       "│  the same essay to grade<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">.</span> One group got an              │ 0.65       │\n",
       "│  these four rapists.\"\\n<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">\\n</span>My own experience at           │ 0.58       │\n",
       "│  received a better grade.<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">\\n</span>\\nBut the most               │ 0.57       │\n",
       "│  mental illness,\" he says<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">.</span>\\n\\nProfessor Graham          │ 0.56       │\n",
       "│  years in Alabama and Georgia<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">.</span> \"Despite what many       │ 0.54       │\n",
       "│  lands of the Baltic nations<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">.</span> Similarly the old barn    │ 0.51       │\n",
       "│  the society at the time<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">.\"</span> and continuing: \"            │ 0.49       │\n",
       "│ ,\" he says.\\n<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">\\n</span>Professor Graham says the                │ 0.49       │\n",
       "│  to jamming and interference<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">.</span> The US, which             │ 0.49       │\n",
       "│  versus these four rapists.\"<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">\\n</span>\\nMy own experience       │ 0.48       │\n",
       "│  (not necessarily cursive<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">).</span> The essay with the          │ 0.48       │\n",
       "│  illness,\" he says.<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">\\n</span>\\nProfessor Graham says            │ 0.48       │\n",
       "│  in treatment over union activity<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">.</span> We felt like our     │ 0.47       │\n",
       "└─────────────────────────────────────────────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                Tokens which most activate this feature                 \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mSequence                                               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│  mattress tied to her back\u001b[1;4;38;5;208m.\"\u001b[0m No charges were filed      │ 1.07       │\n",
       "│  some other type of handwriting\u001b[1;4;38;5;208m.\u001b[0m Cursive proponents say │ 1.00       │\n",
       "│  minimum payment on their accounts\u001b[1;4;38;5;208m.\u001b[0m It��s               │ 0.87       │\n",
       "│  a better grade.\\n\u001b[1;4;38;5;208m\\n\u001b[0mBut the most efficient              │ 0.77       │\n",
       "│  and Pcdh17\u001b[1;4;38;5;208m.\u001b[0m Genes involved in                          │ 0.70       │\n",
       "│  normally only found among Arabs\u001b[1;4;38;5;208m.\u001b[0m<|endoftext|>If you�   │ 0.69       │\n",
       "│  many hundreds of years before\u001b[1;4;38;5;208m.\u001b[0m Huge trade was carried  │ 0.66       │\n",
       "│  the same essay to grade\u001b[1;4;38;5;208m.\u001b[0m One group got an              │ 0.65       │\n",
       "│  these four rapists.\"\\n\u001b[1;4;38;5;208m\\n\u001b[0mMy own experience at           │ 0.58       │\n",
       "│  received a better grade.\u001b[1;4;38;5;208m\\n\u001b[0m\\nBut the most               │ 0.57       │\n",
       "│  mental illness,\" he says\u001b[1;4;38;5;208m.\u001b[0m\\n\\nProfessor Graham          │ 0.56       │\n",
       "│  years in Alabama and Georgia\u001b[1;4;38;5;208m.\u001b[0m \"Despite what many       │ 0.54       │\n",
       "│  lands of the Baltic nations\u001b[1;4;38;5;208m.\u001b[0m Similarly the old barn    │ 0.51       │\n",
       "│  the society at the time\u001b[1;4;38;5;208m.\"\u001b[0m and continuing: \"            │ 0.49       │\n",
       "│ ,\" he says.\\n\u001b[1;4;38;5;208m\\n\u001b[0mProfessor Graham says the                │ 0.49       │\n",
       "│  to jamming and interference\u001b[1;4;38;5;208m.\u001b[0m The US, which             │ 0.49       │\n",
       "│  versus these four rapists.\"\u001b[1;4;38;5;208m\\n\u001b[0m\\nMy own experience       │ 0.48       │\n",
       "│  (not necessarily cursive\u001b[1;4;38;5;208m).\u001b[0m The essay with the          │ 0.48       │\n",
       "│  illness,\" he says.\u001b[1;4;38;5;208m\\n\u001b[0m\\nProfessor Graham says            │ 0.48       │\n",
       "│  in treatment over union activity\u001b[1;4;38;5;208m.\u001b[0m We felt like our     │ 0.47       │\n",
       "└─────────────────────────────────────────────────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = all_tokens\n",
    "feature_idx = random.randint(0, sae.cfg.d_sae)\n",
    "top_acts_indices, top_acts_values = highest_activating_tokens(tokens, model, sae, feature_idx=feature_idx, autoencoder_B=False)\n",
    "display_top_sequences(top_acts_indices, top_acts_values, tokens)\n",
    "del top_acts_indices, tokens, top_acts_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94dde5cc-e06d-47cd-9a5a-ca325af9c085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    1. 1115 - successful, winning, accomplishments\\n    2. 1200 - more\\n\\n    3. 22373 - is/are\\n    4. 21424 - from\\n\\n    5. 18664 - admin, authority\\n\\n    6. 13397 - Man, 2015\\n\\n    7. 19826 - related to laws, rules, code, constitution\\n\\n    8, 11943 - fires on the token after 'every'\\n\\n    9. 19606 - should be interesting to explore\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    1. 1115 - successful, winning, accomplishments\n",
    "    2. 1200 - more\n",
    "\n",
    "    3. 22373 - is/are\n",
    "    4. 21424 - from\n",
    "\n",
    "    5. 18664 - admin, authority\n",
    "\n",
    "    6. 13397 - Man, 2015\n",
    "\n",
    "    7. 19826 - related to laws, rules, code, constitution\n",
    "\n",
    "    8, 11943 - fires on the token after 'every'\n",
    "\n",
    "    9. 19606 - should be interesting to explore\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0041a73c-9d70-4bcf-b593-e87cb5b0ed21",
   "metadata": {},
   "source": [
    "# Love and Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45cf30a6-566d-48d7-b57e-5134ec63d016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <|endoftext|>\n",
      "1 What\n",
      "2  is\n",
      "3  love\n",
      "4 ?\n",
      "5  It\n",
      "6 's\n",
      "7  the\n",
      "8  force\n",
      "9  that\n",
      "10  drives\n",
      "11  all\n",
      "12  of\n",
      "13  human\n",
      "14  actions\n",
      "15 ,\n",
      "16  along\n",
      "17  sides\n",
      "18  greed\n",
      "19  and\n",
      "20  curiosity\n",
      "21 .\n",
      "22  I\n",
      "23  hate\n",
      "24  you\n"
     ]
    }
   ],
   "source": [
    "text = \"What is love? It's the force that drives all of human actions, along sides greed and curiosity. I hate you\"\n",
    "toks = model.to_tokens(text)\n",
    "for i, txt in enumerate(model.to_str_tokens(text)):\n",
    "    print(i, txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38e1ffad-3e98-402d-8f1a-915e67eb4622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of post --> torch.Size([1, 25, 768])\n",
      "Shape of post_reshaped --> torch.Size([25, 768])\n"
     ]
    }
   ],
   "source": [
    "cache = model.run_with_cache(toks, names_filter=[\"blocks.8.hook_resid_post\"])[1]\n",
    "post = cache[\"blocks.8.hook_resid_post\"]\n",
    "print(f\"Shape of post --> {post.shape}\")\n",
    "post_reshaped = einops.rearrange(post, \"batch seq d_model -> (batch seq) d_model\")\n",
    "print(f\"Shape of post_reshaped --> {post_reshaped.shape}\")\n",
    "del cache, post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8a991fd-e9c2-48ee-bbb4-28a396c1f195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 24576])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_cent = post_reshaped - sae.b_dec\n",
    "acts = einops.einsum(\n",
    "    h_cent, sae.W_enc,\n",
    "    \"batch_size n_input_ae, n_input_ae d_sae-> batch_size d_sae\"\n",
    ")\n",
    "del h_cent, post_reshaped\n",
    "acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "910df82c-f5d9-4992-918f-52e951e7f2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10.6604,  4.2406,  4.1642,  3.6488,  3.4874], device='cuda:3',\n",
       "        grad_fn=<TopkBackward0>),\n",
       " tensor([21741, 17760, 24448, 11318,  8294], device='cuda:3'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acts[3].topk(5)\n",
    "\n",
    "top_acts_values, top_acts_indices = acts[3].topk(5)\n",
    "top_acts_values, top_acts_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73cc8566-38b7-458e-a445-36d624d91dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.8020, 5.6348, 3.5676, 3.3164, 3.0523], device='cuda:3',\n",
       "        grad_fn=<TopkBackward0>),\n",
       " tensor([16487, 22262,  3104, 16993, 24153], device='cuda:3'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_acts_values, top_acts_indices = acts[23].topk(5)\n",
    "top_acts_values, top_acts_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "540118b1-788b-45b4-a884-5fcbed903af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_idx = 24448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31cebb0f-b7fb-4f43-9301-69875577b0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83906f78ae6744a7b50d19b4782007a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Steering Output                                                  </span>\n",
       "┌────────────────┬────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
       "│ Normal text    │ When I think about the future, it's almost like I'm in a \"what if?\" moment.                    │\n",
       "│                │                                                                                                │\n",
       "│                │ The world is changing, and we're all still waiting for that moment to happen. And when it      │\n",
       "│                │ happens, I think of how much different things will be in the next                              │\n",
       "├────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered text 0 │ When I think about the future, is itself is itself itself is itself is as itself can becomes   │\n",
       "│                │ itself has must is cannot alone has can is in does, and consists therefore arises and should   │\n",
       "│                │ consists arises produces requires must cannot alone does requires entails implies ultimately   │\n",
       "│                │ alone can becomes cannot necessarily involves ought constitutes                                │\n",
       "├────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered text 1 │ When I think about the future, is is itself is itself is itself will itself alone cannot has   │\n",
       "│                │ is alone cannot consists can itself must becomes has does itself can makes does must           │\n",
       "│                │ necessarily involves arises can and should as arises requires implies creates and exists       │\n",
       "│                │ causes must as does ought requires consists is can entails                                     │\n",
       "├────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered text 2 │ When I think about the future, is itself is itself is itself consists as itself arises in      │\n",
       "│                │ itself alone is has and consists as itself does involves and can must requires arises, is does │\n",
       "│                │ seems comes arises cannot exists cannot therefore necessarily becomes can creates cannot       │\n",
       "│                │ cannot ceases isn in implies implies must depends serves                                       │\n",
       "└────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Steering Output                                                  \u001b[0m\n",
       "┌────────────────┬────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
       "│ Normal text    │ When I think about the future, it's almost like I'm in a \"what if?\" moment.                    │\n",
       "│                │                                                                                                │\n",
       "│                │ The world is changing, and we're all still waiting for that moment to happen. And when it      │\n",
       "│                │ happens, I think of how much different things will be in the next                              │\n",
       "├────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered text 0 │ When I think about the future, is itself is itself itself is itself is as itself can becomes   │\n",
       "│                │ itself has must is cannot alone has can is in does, and consists therefore arises and should   │\n",
       "│                │ consists arises produces requires must cannot alone does requires entails implies ultimately   │\n",
       "│                │ alone can becomes cannot necessarily involves ought constitutes                                │\n",
       "├────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered text 1 │ When I think about the future, is is itself is itself is itself will itself alone cannot has   │\n",
       "│                │ is alone cannot consists can itself must becomes has does itself can makes does must           │\n",
       "│                │ necessarily involves arises can and should as arises requires implies creates and exists       │\n",
       "│                │ causes must as does ought requires consists is can entails                                     │\n",
       "├────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered text 2 │ When I think about the future, is itself is itself is itself consists as itself arises in      │\n",
       "│                │ itself alone is has and consists as itself does involves and can must requires arises, is does │\n",
       "│                │ seems comes arises cannot exists cannot therefore necessarily becomes can creates cannot       │\n",
       "│                │ cannot ceases isn in implies implies must depends serves                                       │\n",
       "└────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_act = 20  # find_max_activation(gemma_2_2b, gemma_2_2b_sae, gemma_2_2b_act_store, feature_idx)\n",
    "\n",
    "prompt = \"When I think about the future,\"\n",
    "\n",
    "no_steering_output = model.generate(prompt, max_new_tokens=50, **GENERATE_KWARGS)\n",
    "\n",
    "# steering_output = generate_with_steering(\n",
    "#     model, sae, prompt, feature_idx, steering_coefficient=10.0\n",
    "# )\n",
    "\n",
    "table = Table(show_header=False, show_lines=True, title=\"Steering Output\")\n",
    "table.add_row(\"Normal text\", no_steering_output)\n",
    "for i in tqdm(range(3)):\n",
    "    table.add_row(\n",
    "        f\"Steered text {i}\",\n",
    "        generate_with_steering(model, sae, prompt, feature_idx, steering_coefficient=80.0),\n",
    "    )\n",
    "rprint(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d4554c-c2b7-488c-89b6-a1037139c24f",
   "metadata": {},
   "source": [
    "# Golden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d927a426-4390-4392-a56e-9d26029a2af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 9]),\n",
       " torch.Size([1, 9]),\n",
       " ['<|endoftext|>',\n",
       "  'event',\n",
       "  'ually',\n",
       "  ',',\n",
       "  ' it',\n",
       "  ' kills',\n",
       "  ' the',\n",
       "  ' golden',\n",
       "  ' goose'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\n",
    "    \"her performance on the golden globe award.\",\n",
    "    \"my favourite team is the golden state warriors\",\n",
    "    'eventually, it kills the golden goose'\n",
    "]\n",
    "\n",
    "text_toks = model.to_tokens(text)\n",
    "text_toks.shape, text_toks[0].unsqueeze(0).shape, model.to_str_tokens(text[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f061ec34-0ed7-4686-96cf-7c43788067d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_act_and_val(text_toks):\n",
    "    cache = model.run_with_cache(text_toks, names_filter=[\"blocks.8.hook_resid_post\"])[1]\n",
    "    post = cache[\"blocks.8.hook_resid_post\"]\n",
    "    print(f\"Shape of post --> {post.shape}\")\n",
    "    post_reshaped = einops.rearrange(post, \"batch seq d_model -> (batch seq) d_model\")\n",
    "    print(f\"Shape of post_reshaped --> {post_reshaped.shape}\")\n",
    "    del cache, post\n",
    "    h_cent = post_reshaped - sae.b_dec\n",
    "    acts = einops.einsum(\n",
    "        h_cent, sae.W_enc,\n",
    "        \"batch_size n_input_ae, n_input_ae d_sae-> batch_size d_sae\"\n",
    "    )\n",
    "    del h_cent, post_reshaped\n",
    "    acts.shape\n",
    "    act_val, idx = acts[7].topk(5)\n",
    "    print(act_val, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "59f6ac06-de4d-418a-a3aa-834aa098efb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of post --> torch.Size([1, 9, 768])\n",
      "Shape of post_reshaped --> torch.Size([9, 768])\n",
      "tensor([12.4077,  2.9669,  2.9256,  2.8421,  2.2421], device='cuda:2',\n",
      "       grad_fn=<TopkBackward0>) tensor([22693, 11632, 24323, 14709,  7713], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "get_act_and_val(text_toks[2].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863d2cd-cf26-41ed-a930-cfc8ff53ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "[22693, 14709, 11632, 24323,  4822]\n",
    "[22693, 14709, 11632, 23671, 24323]\n",
    "[22693, 11632, 24323, 14709,  7713]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a2ecdbf2-a046-4316-bd73-c653e5c78d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2944b47937450bb12fe0bf80360136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([15.5017], device='cuda:2', grad_fn=<TopkBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_max_activation(model=model, sae=sae, act_store=actstore, feature_idx=22693, num_batches=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e9a4422d-d37c-4fe4-a48b-6bd93c342587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e387719c01401d99af1ac406f0ed2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Steering Output                                                  </span>\n",
       "┌────────────────┬────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
       "│ Normal text    │ How about that spoon?, I'm sure you can guess what's going to happen.                          │\n",
       "│                │                                                                                                │\n",
       "│                │ The following are the results of a study in which participants were asked to pick up a spoon   │\n",
       "│                │ from an empty handbag and place it on the table next to them. The researchers found            │\n",
       "├────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered text 0 │ How about that spoon?, the last one was a little bit of a \"is it really all right?\" moment.    │\n",
       "│                │                                                                                                │\n",
       "│                │ It's not like I'm talking about the Golden Age of Snailfish, where we were able to get around  │\n",
       "│                │ the golden age of golden gills                                                                 │\n",
       "├────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered text 1 │ How about that spoon?, I bet you did, and you were able to find a way to get this one from the │\n",
       "│                │ same hole as it was supposed to be.                                                            │\n",
       "│                │                                                                                                │\n",
       "│                │ Famous Golden Age Wine Sauvage: A Golden Age Golden Age golden age golden age golden age       │\n",
       "├────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered text 2 │ How about that spoon?, don't you think?                                                        │\n",
       "│                │                                                                                                │\n",
       "│                │ I mean, I'm a little disappointed to see it in the same old form as the last one. It's an      │\n",
       "│                │ eggplant bocage, which is a golden-rod and contains the Golden Gate State Curry                │\n",
       "└────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Steering Output                                                  \u001b[0m\n",
       "┌────────────────┬────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
       "│ Normal text    │ How about that spoon?, I'm sure you can guess what's going to happen.                          │\n",
       "│                │                                                                                                │\n",
       "│                │ The following are the results of a study in which participants were asked to pick up a spoon   │\n",
       "│                │ from an empty handbag and place it on the table next to them. The researchers found            │\n",
       "├────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered text 0 │ How about that spoon?, the last one was a little bit of a \"is it really all right?\" moment.    │\n",
       "│                │                                                                                                │\n",
       "│                │ It's not like I'm talking about the Golden Age of Snailfish, where we were able to get around  │\n",
       "│                │ the golden age of golden gills                                                                 │\n",
       "├────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered text 1 │ How about that spoon?, I bet you did, and you were able to find a way to get this one from the │\n",
       "│                │ same hole as it was supposed to be.                                                            │\n",
       "│                │                                                                                                │\n",
       "│                │ Famous Golden Age Wine Sauvage: A Golden Age Golden Age golden age golden age golden age       │\n",
       "├────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered text 2 │ How about that spoon?, don't you think?                                                        │\n",
       "│                │                                                                                                │\n",
       "│                │ I mean, I'm a little disappointed to see it in the same old form as the last one. It's an      │\n",
       "│                │ eggplant bocage, which is a golden-rod and contains the Golden Gate State Curry                │\n",
       "└────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_act = 5  # find_max_activation(gemma_2_2b, gemma_2_2b_sae, gemma_2_2b_act_store, feature_idx)\n",
    "feature_idx = 22693\n",
    "prompt = \"How about that spoon?,\"\n",
    "\n",
    "no_steering_output = model.generate(prompt, max_new_tokens=50, **GENERATE_KWARGS)\n",
    "\n",
    "steering_output = generate_with_steering(\n",
    "    model, sae, prompt, feature_idx, max_act, steering_strength=3.0\n",
    ")\n",
    "\n",
    "table = Table(show_header=False, show_lines=True, title=\"Steering Output\")\n",
    "table.add_row(\"Normal text\", no_steering_output)\n",
    "for i in tqdm(range(3)):\n",
    "    table.add_row(\n",
    "        f\"Steered text {i}\",\n",
    "        generate_with_steering(model, sae, prompt, feature_idx, max_act, steering_strength=2.5),\n",
    "    )\n",
    "rprint(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57777e4-52e6-4c7c-8964-2d99eeecd517",
   "metadata": {},
   "source": [
    "# Gemma 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5298535e-52fe-4d21-8d2a-49dcd0994828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1697365f834417bae0f0d61a5c125c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b-it into HookedTransformer\n",
      "Moving model to device:  cuda:2\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained('gemma-2-2b-it').to('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16083e9d-ca2f-4e99-9d17-80fc39227d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ac700d96a146dabe2b0777861897f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)h_37K/blocks.23.hook_resid_post/cfg.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1fad2d9b8e44ea188d3789a6bb00eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sae_weights.safetensors:   0%|          | 0.00/680M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SAE(\n",
       "  (activation_fn): ReLU()\n",
       "  (hook_sae_input): HookPoint()\n",
       "  (hook_sae_acts_pre): HookPoint()\n",
       "  (hook_sae_acts_post): HookPoint()\n",
       "  (hook_sae_output): HookPoint()\n",
       "  (hook_sae_recons): HookPoint()\n",
       "  (hook_sae_error): HookPoint()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\"israel-adewuyi/Gemma2-2B-SAE\", \"resid_post/layer_23/width_37K/blocks.23.hook_resid_post\")\n",
    "sae.to('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f944aeea-f2ea-4ecd-bdf7-e3eeea2cfd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 36864-L1-5-LR-5e-05-Tokens-2.560e+07\n",
      "n_tokens_per_buffer (millions): 0.524288\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 50000\n",
      "Total wandb updates: 2500\n",
      "n_tokens_per_feature_sampling_window (millions): 262.144\n",
      "n_tokens_per_dead_feature_window (millions): 262.144\n",
      "We will reset the sparsity calculation 50 times.\n",
      "Number tokens in sparsity calculation window: 5.12e+05\n"
     ]
    }
   ],
   "source": [
    "total_training_steps = 50_000\n",
    "batch_size = 512\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = 0\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "l1_warm_up_steps = total_training_steps // 20  # 5% of training\n",
    "\n",
    "layer = 23\n",
    "width = 37\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"gemma-2-2b-it\", \n",
    "    hook_name=\"blocks.23.hook_resid_post\", \n",
    "    hook_layer=23,  \n",
    "    d_in=2304,  \n",
    "    dataset_path=\"Skylion007/openwebtext\",  \n",
    "    # dataset_path=\"NeelNanda/c4-code-20k\",\n",
    "    is_dataset_tokenized=False,\n",
    "    streaming=True, \n",
    "    # SAE Parameters\n",
    "    architecture=\"gated\",\n",
    "    mse_loss_normalization=None,  \n",
    "    expansion_factor=16,  \n",
    "    b_dec_init_method=\"zeros\",  # The geometric median can be used to initialize the decoder weights.\n",
    "    apply_b_dec_to_input=False,\n",
    "    normalize_sae_decoder=False,\n",
    "    scale_sparsity_penalty_by_decoder_norm=True,\n",
    "    decoder_heuristic_init=True,\n",
    "    init_encoder_as_decoder_transpose=True,\n",
    "    normalize_activations=\"expected_average_only_in\",\n",
    "    # Training Parameters\n",
    "    lr=5e-5,  \n",
    "    adam_beta1=0.9, \n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n",
    "    lr_warm_up_steps=lr_warm_up_steps,  \n",
    "    lr_decay_steps=lr_decay_steps,  \n",
    "    l1_coefficient=5,  \n",
    "    l1_warm_up_steps=l1_warm_up_steps, \n",
    "    lp_norm=1.0,  \n",
    "    train_batch_size_tokens=batch_size,\n",
    "    context_size=512, #Larger is better but slower.\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=64, \n",
    "    training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n",
    "    store_batch_size_prompts=16,\n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=False,  # we don't use ghost grads anymore.\n",
    "    feature_sampling_window=1000, \n",
    "    dead_feature_window=1000,  \n",
    "    dead_feature_threshold=1e-4,  \n",
    "    # WANDB\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"Autoencoders_sae-lens\",\n",
    "    wandb_log_frequency=20,\n",
    "    eval_every_n_wandb_logs=10,\n",
    "    # Misc\n",
    "    device='cuda:2',\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c948729-379b-4cea-a39b-194a150cb6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n"
     ]
    }
   ],
   "source": [
    "actstore = ActivationsStore.from_config(cfg=cfg,model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3df5bb17-89a1-46f0-aba5-69904acd7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = actstore.get_batch_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb70f75c-851b-4329-b224-d01ff2d2db19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature index is 21789\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                    Tokens which most activate this feature                     </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Sequence                                                        </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│  trivia.\\n\\nFor the<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> next</span> decade, the true                       │ 0.96       │\n",
       "│  the demonstration — on the<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> part</span> of the students,               │ 0.75       │\n",
       "│  heart failure and for the<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> majority</span> there is no cure            │ 0.67       │\n",
       "│ /AP Photo\\n\\nThe<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> Kush</span>ner family is also                         │ 0.65       │\n",
       "│  with Islam.”\\n\\nBut<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">,</span> he added, “                               │ 0.61       │\n",
       "│  information that might interest the<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> Kush</span>ners. (“The            │ 0.58       │\n",
       "│  Wally Edge. The real<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> Edge</span> was a newspaper publisher            │ 0.58       │\n",
       "│ . (“Among those of<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> us</span> who pay inordinate                        │ 0.56       │\n",
       "│  personality. Wildstein proves<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> him</span> wrong, winning a             │ 0.55       │\n",
       "│  DuHaime and the<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> other</span> Franks-campaign veteran                  │ 0.55       │\n",
       "│  the township first and his<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> own</span> political ambitions second,”    │ 0.53       │\n",
       "│  from Livingston. He and<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> his</span> brother, a Republican              │ 0.53       │\n",
       "│ stein ever did. Communicating<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> almost</span> exclusively by AOL Instant │ 0.53       │\n",
       "│  are furious. Someone leaks<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> Edge</span>’s true identity                │ 0.53       │\n",
       "│ , sources say, the<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> Kush</span>ners are furious.                        │ 0.52       │\n",
       "│  I was always one of<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> his</span> favorites.”) Wildstein                 │ 0.51       │\n",
       "│  around the body effectively.<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> Most</span> commonly this is due         │ 0.51       │\n",
       "│ . In order to demonstrate<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> his</span> centrist appeal,                  │ 0.51       │\n",
       "│  in New York (he<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> already</span> owns the New York                      │ 0.50       │\n",
       "│  and the Institute for Israel<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> Studies</span> in conducting courses and │ 0.49       │\n",
       "└─────────────────────────────────────────────────────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                    Tokens which most activate this feature                     \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mSequence                                                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│  trivia.\\n\\nFor the\u001b[1;4;38;5;208m next\u001b[0m decade, the true                       │ 0.96       │\n",
       "│  the demonstration — on the\u001b[1;4;38;5;208m part\u001b[0m of the students,               │ 0.75       │\n",
       "│  heart failure and for the\u001b[1;4;38;5;208m majority\u001b[0m there is no cure            │ 0.67       │\n",
       "│ /AP Photo\\n\\nThe\u001b[1;4;38;5;208m Kush\u001b[0mner family is also                         │ 0.65       │\n",
       "│  with Islam.”\\n\\nBut\u001b[1;4;38;5;208m,\u001b[0m he added, “                               │ 0.61       │\n",
       "│  information that might interest the\u001b[1;4;38;5;208m Kush\u001b[0mners. (“The            │ 0.58       │\n",
       "│  Wally Edge. The real\u001b[1;4;38;5;208m Edge\u001b[0m was a newspaper publisher            │ 0.58       │\n",
       "│ . (“Among those of\u001b[1;4;38;5;208m us\u001b[0m who pay inordinate                        │ 0.56       │\n",
       "│  personality. Wildstein proves\u001b[1;4;38;5;208m him\u001b[0m wrong, winning a             │ 0.55       │\n",
       "│  DuHaime and the\u001b[1;4;38;5;208m other\u001b[0m Franks-campaign veteran                  │ 0.55       │\n",
       "│  the township first and his\u001b[1;4;38;5;208m own\u001b[0m political ambitions second,”    │ 0.53       │\n",
       "│  from Livingston. He and\u001b[1;4;38;5;208m his\u001b[0m brother, a Republican              │ 0.53       │\n",
       "│ stein ever did. Communicating\u001b[1;4;38;5;208m almost\u001b[0m exclusively by AOL Instant │ 0.53       │\n",
       "│  are furious. Someone leaks\u001b[1;4;38;5;208m Edge\u001b[0m’s true identity                │ 0.53       │\n",
       "│ , sources say, the\u001b[1;4;38;5;208m Kush\u001b[0mners are furious.                        │ 0.52       │\n",
       "│  I was always one of\u001b[1;4;38;5;208m his\u001b[0m favorites.”) Wildstein                 │ 0.51       │\n",
       "│  around the body effectively.\u001b[1;4;38;5;208m Most\u001b[0m commonly this is due         │ 0.51       │\n",
       "│ . In order to demonstrate\u001b[1;4;38;5;208m his\u001b[0m centrist appeal,                  │ 0.51       │\n",
       "│  in New York (he\u001b[1;4;38;5;208m already\u001b[0m owns the New York                      │ 0.50       │\n",
       "│  and the Institute for Israel\u001b[1;4;38;5;208m Studies\u001b[0m in conducting courses and │ 0.49       │\n",
       "└─────────────────────────────────────────────────────────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_idx = random.randint(0, sae.cfg.d_sae)\n",
    "top_acts_indices, top_acts_values = highest_activating_tokens(tokens, model, sae, feature_idx=feature_idx, autoencoder_B=False)\n",
    "display_top_sequences(top_acts_indices, top_acts_values, tokens)\n",
    "del top_acts_indices, top_acts_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d92db558-a196-436c-8fdf-24b9633b4082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 9]),\n",
       " torch.Size([1, 9]),\n",
       " ['<bos>',\n",
       "  'her',\n",
       "  ' performance',\n",
       "  ' on',\n",
       "  ' the',\n",
       "  ' golden',\n",
       "  ' globe',\n",
       "  ' award',\n",
       "  '.'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\n",
    "    \"her performance on the golden globe award.\",\n",
    "    \"my favourite team is the golden state warriors\",\n",
    "    'eventually, it kills the golden goose'\n",
    "]\n",
    "\n",
    "text_toks = model.to_tokens(text)\n",
    "text_toks.shape, text_toks[0].unsqueeze(0).shape, model.to_str_tokens(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "200174be-ee9f-4607-9a58-b723df9858e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_act_and_val(text_toks):\n",
    "    cache = model.run_with_cache(text_toks, names_filter=[\"blocks.8.hook_resid_post\"])[1]\n",
    "    post = cache[\"blocks.8.hook_resid_post\"]\n",
    "    print(f\"Shape of post --> {post.shape}\")\n",
    "    post_reshaped = einops.rearrange(post, \"batch seq d_model -> (batch seq) d_model\")\n",
    "    print(f\"Shape of post_reshaped --> {post_reshaped.shape}\")\n",
    "    del cache, post\n",
    "    h_cent = post_reshaped - sae.b_dec\n",
    "    acts = einops.einsum(\n",
    "        h_cent, sae.W_enc,\n",
    "        \"batch_size n_input_ae, n_input_ae d_sae-> batch_size d_sae\"\n",
    "    )\n",
    "    del h_cent, post_reshaped\n",
    "    acts.shape\n",
    "    act_val, idx = acts[5].topk(5)\n",
    "    print(act_val, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e394948-9a6a-4cd9-a4b8-ee16db06dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "[12326, 35741, 22727, 26171, 11608]\n",
    "[35741, 12326, 22727, 26171,  7116]\n",
    "[12326, 35741, 22727, 26171, 15298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d97a3641-82d1-425c-b97f-8a45e53ba8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of post --> torch.Size([1, 9, 2304])\n",
      "Shape of post_reshaped --> torch.Size([9, 2304])\n",
      "tensor([1.1850, 1.0809, 0.8252, 0.7400, 0.7004], device='cuda:2',\n",
      "       grad_fn=<TopkBackward0>) tensor([12326, 35741, 22727, 26171, 15298], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "get_act_and_val(text_toks[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "339cc46a-4ec5-4fb6-88b0-19ddd3fa935f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ca4314af5f4d038749bb5e516c184f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.2520], device='cuda:2', grad_fn=<TopkBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_max_activation(model=model, sae=sae, act_store=actstore, feature_idx=22727, num_batches=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c89a12e2-2603-421e-8531-e98b603634e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b65525566924d95bd8223d978ccd9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating steered examples...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Steering Output                                                  </span>\n",
       "┌────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
       "│ Normal     │ When I look at myself in the mirror, I see a reflection of my past and present. It's a constant    │\n",
       "│            │ reminder of who I am, where I've been, and where I'm going. But sometimes, it can feel like just a │\n",
       "│            │ distorted version of reality.                                                                      │\n",
       "│            │                                                                                                    │\n",
       "│            │ I think                                                                                            │\n",
       "├────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered #0 │ When I look at myself in the mirror, I see a brown haired girl with brown eyes. I mean, that' age  │\n",
       "│            │ old age-old question: what am I?↵↵I am not sure what age you are, but it seems like you're asking  │\n",
       "│            │ a question that is meant to be playful                                                             │\n",
       "├────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered #1 │ When I look at myself in the mirror, I see a stranger. Years have passed, moments have been lived, │\n",
       "│            │ and age has touched me. The face that smiles back may not be the one I knew years ago, but it'     │\n",
       "│            │ years ago. It still holds memories of laughter and tears, victories and                            │\n",
       "├────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered #2 │ When I look at myself in the mirror, I see a reflection of my age and experience. Age has etched   │\n",
       "│            │ lines on my face, and years have left their mark on my hair. Experience has shaped me into the     │\n",
       "│            │ person I am today, filled with knowledge and wisdom gleaned from years of living. But age          │\n",
       "└────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Steering Output                                                  \u001b[0m\n",
       "┌────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
       "│ Normal     │ When I look at myself in the mirror, I see a reflection of my past and present. It's a constant    │\n",
       "│            │ reminder of who I am, where I've been, and where I'm going. But sometimes, it can feel like just a │\n",
       "│            │ distorted version of reality.                                                                      │\n",
       "│            │                                                                                                    │\n",
       "│            │ I think                                                                                            │\n",
       "├────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered #0 │ When I look at myself in the mirror, I see a brown haired girl with brown eyes. I mean, that' age  │\n",
       "│            │ old age-old question: what am I?↵↵I am not sure what age you are, but it seems like you're asking  │\n",
       "│            │ a question that is meant to be playful                                                             │\n",
       "├────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered #1 │ When I look at myself in the mirror, I see a stranger. Years have passed, moments have been lived, │\n",
       "│            │ and age has touched me. The face that smiles back may not be the one I knew years ago, but it'     │\n",
       "│            │ years ago. It still holds memories of laughter and tears, victories and                            │\n",
       "├────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered #2 │ When I look at myself in the mirror, I see a reflection of my age and experience. Age has etched   │\n",
       "│            │ lines on my face, and years have left their mark on my hair. Experience has shaped me into the     │\n",
       "│            │ person I am today, filled with knowledge and wisdom gleaned from years of living. But age          │\n",
       "└────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_act = 80  # find_max_activation(gemma_2_2b, gemma_2_2b_sae, gemma_2_2b_act_store, feature_idx)\n",
    "feature_idx = 12326\n",
    "prompt = \"When I look at myself in the mirror, I see\"\n",
    "\n",
    "no_steering_output = model.generate(prompt, max_new_tokens=50, **GENERATE_KWARGS)\n",
    "\n",
    "table = Table(show_header=False, show_lines=True, title=\"Steering Output\")\n",
    "table.add_row(\"Normal\", no_steering_output)\n",
    "for i in tqdm(range(3), \"Generating steered examples...\"):\n",
    "    table.add_row(\n",
    "        f\"Steered #{i}\",\n",
    "        generate_with_steering(\n",
    "            model,\n",
    "            sae,\n",
    "            prompt,\n",
    "            feature_idx,\n",
    "            steering_coefficient=60,  # roughly 1.5-2x the latent's max activation\n",
    "        ).replace(\"\\n\", \"↵\"),\n",
    "    )\n",
    "rprint(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564fb776-7b9b-4fe7-ba32-93631fbf08b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GemmaScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "611a88ce-fb83-4148-a016-b6025eaf5384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING_GEMMA = os.environ.get(\"HUGGINGFACE_KEY\")\n",
    "device='cuda:3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a43f760f-a234-4d50-9796-f7e3f4e43312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c867a2d8e51444baa27bbc13875e598b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-9b-it into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f2937d530946b0b5ef69b42fe6be61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/470M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_2_2b = HookedSAETransformer.from_pretrained(\"gemma-2-9b-it\", device=device)\n",
    "\n",
    "gemmascope_sae_release = \"gemma-scope-9b-it-res\"\n",
    "gemmascope_sae_id = \"layer_20/width_16k/average_l0_14\"\n",
    "\n",
    "gemma_2_2b_sae = SAE.from_pretrained(gemmascope_sae_release, gemmascope_sae_id, device=str(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33efb920-86e5-4e32-9a6a-0d857b35eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_idx = 12082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82dc238c-8a7e-4e96-854e-2a4a5962d60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260a3eab72c24cf6ac5c9d051731aa88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating steered examples...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Steering Output                                                  </span>\n",
       "┌────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
       "│ Normal     │ When I look at myself in the mirror, I see a reflection of my past, present, and future.           │\n",
       "│            │                                                                                                    │\n",
       "│            │ My past is etched into my wrinkles and laugh lines, a testament to the joys and sorrows I've       │\n",
       "│            │ experienced. My eyes hold the memories of laughter shared with loved ones and tears shed during    │\n",
       "├────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered #0 │ When I look at myself in the mirror, I see my skin and,ness..ness. air around air around↵↵         │\n",
       "│            │ environment   ness surrounding my skin surface↵↵↵ surroundings ↵↵   iness   warm  warmth↵↵ warmth  │\n",
       "│            │ humidity↵↵↵↵   coolness↵↵↵ coldness↵↵↵↵    conditions  environment↵ environments atmosphere        │\n",
       "│            │ surrounding air                                                                                    │\n",
       "├────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered #1 │ When I look at myself in the mirror, I see. and↵↵ness.↵↵ness... ...... air....  warmth..  ↵↵       │\n",
       "│            │ air...   coolness....↵↵↵ () coolness, heat;. cold air; humid areas↵↵  - warmth moistness     heat  │\n",
       "│            │ vapor↵↵↵ moist warm                                                                                │\n",
       "├────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered #2 │ When I look at myself in the mirror, I see....↵↵: !↵↵ isness, air, heat clinging to warmth of      │\n",
       "│            │ skin. and air clinging heat warmth of skin warmness from cool fingers against skin coolness but    │\n",
       "│            │ not coldness chills lingering from the air cool   cold chill clinging  ↵↵…                         │\n",
       "└────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Steering Output                                                  \u001b[0m\n",
       "┌────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
       "│ Normal     │ When I look at myself in the mirror, I see a reflection of my past, present, and future.           │\n",
       "│            │                                                                                                    │\n",
       "│            │ My past is etched into my wrinkles and laugh lines, a testament to the joys and sorrows I've       │\n",
       "│            │ experienced. My eyes hold the memories of laughter shared with loved ones and tears shed during    │\n",
       "├────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered #0 │ When I look at myself in the mirror, I see my skin and,ness..ness. air around air around↵↵         │\n",
       "│            │ environment   ness surrounding my skin surface↵↵↵ surroundings ↵↵   iness   warm  warmth↵↵ warmth  │\n",
       "│            │ humidity↵↵↵↵   coolness↵↵↵ coldness↵↵↵↵    conditions  environment↵ environments atmosphere        │\n",
       "│            │ surrounding air                                                                                    │\n",
       "├────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered #1 │ When I look at myself in the mirror, I see. and↵↵ness.↵↵ness... ...... air....  warmth..  ↵↵       │\n",
       "│            │ air...   coolness....↵↵↵ () coolness, heat;. cold air; humid areas↵↵  - warmth moistness     heat  │\n",
       "│            │ vapor↵↵↵ moist warm                                                                                │\n",
       "├────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Steered #2 │ When I look at myself in the mirror, I see....↵↵: !↵↵ isness, air, heat clinging to warmth of      │\n",
       "│            │ skin. and air clinging heat warmth of skin warmness from cool fingers against skin coolness but    │\n",
       "│            │ not coldness chills lingering from the air cool   cold chill clinging  ↵↵…                         │\n",
       "└────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"When I look at myself in the mirror, I see\"\n",
    "\n",
    "no_steering_output = gemma_2_2b.generate(prompt, max_new_tokens=50, **GENERATE_KWARGS)\n",
    "\n",
    "table = Table(show_header=False, show_lines=True, title=\"Steering Output\")\n",
    "table.add_row(\"Normal\", no_steering_output)\n",
    "for i in tqdm(range(3), \"Generating steered examples...\"):\n",
    "    table.add_row(\n",
    "        f\"Steered #{i}\",\n",
    "        generate_with_steering(\n",
    "            gemma_2_2b,\n",
    "            gemma_2_2b_sae,\n",
    "            prompt,\n",
    "            latent_idx,\n",
    "            steering_coefficient=240.0,  # roughly 1.5-2x the latent's max activation\n",
    "        ).replace(\"\\n\", \"↵\"),\n",
    "    )\n",
    "rprint(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MechInterp",
   "language": "python",
   "name": "mechintep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
