{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d289d0ea-151f-48f1-9d33-09f0454c58eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install transformer-lens jaxtyping datasets\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9953e30-3858-45bd-ab9c-7bca069c0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import wandb\n",
    "import torch\n",
    "import einops\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch import Tensor\n",
    "from rich.table import Table\n",
    "from jaxtyping import Float, Int\n",
    "from rich import print as rprint\n",
    "from typing import Callable, Tuple\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from transformer_lens import HookedTransformer\n",
    "from torch.distributions.categorical import Categorical\n",
    "from transformer_lens.utils import (\n",
    "    get_act_name,\n",
    "    load_dataset,\n",
    "    tokenize_and_concatenate,\n",
    "    download_file_from_hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de21d56b-8d0a-44d2-9a74-2eca6d42d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_in: int = 768\n",
    "    dict_mult: int = 32\n",
    "    d_sae: int = field(init=False)\n",
    "    tied_weights: bool = False\n",
    "    layer: int = 8\n",
    "    device: str = 'cuda:3'\n",
    "    l1_coefficient: int = 8e-5\n",
    "    weight_normalize_eps: float = 1e-8\n",
    "\n",
    "    seq_len: int = 128\n",
    "    batch_size: int = 4096\n",
    "    component_name: str = \"resid_post\"\n",
    "    act_name: str = field(init=False)\n",
    "    \n",
    "    buffer_mult: int = 384\n",
    "    buffer_size: int = field(init=False)\n",
    "    buffer_batches: int = field(init=False)\n",
    "    model_batch_size: int =  field(init=False)\n",
    "\n",
    "    log_freq: int = 50\n",
    "    lr:float = 4e-4\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d_sae = self.d_in * self.dict_mult\n",
    "        self.buffer_size = self.batch_size * self.buffer_mult\n",
    "        self.buffer_batches = self.buffer_size // self.seq_len\n",
    "        self.model_batch_size =  (self.batch_size // self.seq_len * 16)\n",
    "        self.act_name = get_act_name(self.component_name, self.layer)\n",
    "\n",
    "    \n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad8672aa-1ef8-46fa-9a59-9de9803b4936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/israel/mechintep/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    device=cfg.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "045e23f6-2481-481e-8dc3-27ab213d08f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens shape:  torch.Size([325017, 128])\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"NeelNanda/c4-code-20k\", split=\"train\")\n",
    "tokenized_data = tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "all_tokens = tokenized_data[\"tokens\"]\n",
    "del data, tokenized_data\n",
    "print(\"Tokens shape: \", all_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39be7fce-031d-462f-a95c-fdc9714ae95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_lr(*_):\n",
    "    return 1.0\n",
    "\n",
    "class Buffer():\n",
    "    def __init__(self, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "        self.buffer = torch.zeros((self.cfg.buffer_size, self.cfg.d_in), requires_grad=False, dtype=torch.bfloat16).to(self.cfg.device)\n",
    "        self.pointer = 0\n",
    "        self.token_pointer = 0\n",
    "        self.first = True  \n",
    "        self.refresh()\n",
    "    \n",
    "    def refresh(self):\n",
    "        self.pointer = 0\n",
    "        with torch.autocast(\"cuda\", torch.bfloat16):\n",
    "            if self.first:\n",
    "                num_batches = self.cfg.buffer_batches\n",
    "            else:\n",
    "                num_batches = self.cfg.buffer_batches // 2\n",
    "\n",
    "            self.first = False\n",
    "\n",
    "            for _ in range(0, num_batches, self.cfg.model_batch_size):\n",
    "                tokens = all_tokens[self.token_pointer: self.token_pointer + self.cfg.model_batch_size]\n",
    "\n",
    "                _, cache = model.run_with_cache(tokens, stop_at_layer=self.cfg.layer + 1, names_filter=self.cfg.act_name)\n",
    "\n",
    "                # print(list(cache.keys()))\n",
    "                acts = einops.rearrange(\n",
    "                    cache[self.cfg.act_name],\n",
    "                    \"batch seq d_model -> (batch seq) d_model\"\n",
    "                )\n",
    "\n",
    "                \n",
    "                del cache\n",
    "                \n",
    "                self.buffer[self.pointer: self.pointer+acts.shape[0]] = acts\n",
    "                self.pointer += acts.shape[0]\n",
    "                self.token_pointer += self.cfg.model_batch_size\n",
    "\n",
    "                if self.token_pointer + self.cfg.model_batch_size >= all_tokens.shape[0]:\n",
    "                    self.token_pointer = 0\n",
    "            \n",
    "        self.pointer = 0\n",
    "        self.buffer = self.buffer[torch.randperm(self.buffer.shape[0]).to(self.cfg.device)]\n",
    "        \n",
    "    def next(self) -> Int[Tensor, \"batch_seq d_model\"]:\n",
    "        out = self.buffer[self.pointer: self.pointer + self.cfg.batch_size]\n",
    "        self.pointer += self.cfg.batch_size\n",
    "\n",
    "        if self.pointer + self.cfg.batch_size > self.buffer.shape[0] // 2:\n",
    "            self.refresh()\n",
    "\n",
    "        return out\n",
    "\n",
    "buffer = Buffer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4acbaacc-6e5a-4300-a5d8-562e949b1b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 cfg: Config,\n",
    "                 model \n",
    "                 ): \n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.model = model\n",
    "\n",
    "        self.W_enc = nn.Parameter(nn.init.xavier_uniform_(torch.empty(self.cfg.d_in, self.cfg.d_sae)))\n",
    "        \n",
    "        self.b_enc = nn.Parameter(torch.zeros(self.cfg.d_sae))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(self.cfg.d_in))\n",
    "\n",
    "        if self.cfg.tied_weights:\n",
    "            self._W_dec = None\n",
    "        else:\n",
    "            self._W_dec = nn.Parameter(nn.init.xavier_uniform_(torch.empty(self.cfg.d_sae, self.cfg.d_in)))\n",
    "    \n",
    "    @property\n",
    "    def W_dec(self) -> Float[Tensor, \"d_sae d_in\"]:\n",
    "        return self._W_dec if self._W_dec is not None else self.W_enc.transpose(1, 0)\n",
    "    \n",
    "    @property\n",
    "    def W_dec_normalized(self) -> Float[Tensor, \"d_sae d_in\"]:\n",
    "        \"\"\"Returns decoder weights, normalized over the autoencoder input dimension.\"\"\"\n",
    "        return self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def forward(self,\n",
    "                h: Float[Tensor, \"batch_seq d_in\"] \n",
    "    ) -> tuple[\n",
    "            dict[str, Float[Tensor, \"batch_seq\"]],\n",
    "            Float[Tensor, \"\"],\n",
    "            Float[Tensor, \"batch_seq d_sae\"],\n",
    "            Float[Tensor, \"batch_seq d_in\"]\n",
    "        ]:\n",
    "        assert h.shape[1] == self.cfg.d_in \n",
    "\n",
    "        # print(f\"This is the shape of h {h.shape}\")\n",
    "\n",
    "        acts = F.relu(\n",
    "            einops.einsum(\n",
    "                (h - self.b_dec), self.W_enc,\n",
    "                \"batch_seq d_in, d_in d_sae -> batch_seq d_sae\"\n",
    "            ) + self.b_enc\n",
    "        )\n",
    "\n",
    "        h_reconstructed = einops.einsum(\n",
    "            acts, self.W_dec,\n",
    "            \"batch_seq d_sae, d_sae d_in -> batch_seq d_in\"\n",
    "        )\n",
    "\n",
    "        assert h_reconstructed.shape == h.shape\n",
    "        # h_reconstructed is batch_seq d_in\n",
    "        L_reconstruction = ((h - h_reconstructed) ** 2).mean(dim=-1)\n",
    "        L_sparsity = acts.abs().sum(dim=-1)\n",
    "    \n",
    "        loss_dict = {\n",
    "            \"L_reconstruction\": L_reconstruction,\n",
    "            \"L_sparsity\": L_sparsity\n",
    "        }\n",
    "\n",
    "        loss = (L_reconstruction + (self.cfg.l1_coefficient * L_sparsity)).mean()\n",
    "\n",
    "        return loss_dict, loss, acts, h_reconstructed\n",
    "\n",
    "\n",
    "    def optimize(\n",
    "            self, \n",
    "            steps: int = 30_000,\n",
    "            log_freq: int = 50,\n",
    "            lr_scale: Callable[[int, int], float] = constant_lr,\n",
    "            resample_freq: int = 2500,\n",
    "            resample_window: int = 500,\n",
    "            resample_scale: float = 0.5\n",
    "\n",
    "    ):\n",
    "        assert resample_window <= resample_freq\n",
    "\n",
    "        name = f\"L{self.cfg.layer}_{self.cfg.d_sae}_L1-{self.cfg.l1_coefficient}_Lr-{self.cfg.lr}\"\n",
    "\n",
    "        wandb.init(project=\"Autoencoders\", name=name)\n",
    "\n",
    "        optimizer = torch.optim.Adam(list(self.parameters()), lr=self.cfg.lr, betas=(0.0, 0.999))\n",
    "        progress_bar = tqdm(range(steps))\n",
    "        frac_active_list = []\n",
    "\n",
    "        for step in progress_bar:\n",
    "            if ((step + 1) % resample_freq == 0):\n",
    "                frac_active_in_window = torch.stack(frac_active_list[-resample_window:], dim=0)\n",
    "                self.resample_advanced(frac_active_in_window, resample_scale, self.cfg.batch_size)\n",
    "\n",
    "            # Update learning rate\n",
    "            step_lr = self.cfg.lr * lr_scale(step, steps)\n",
    "            for group in optimizer.param_groups:\n",
    "                group[\"lr\"] = step_lr\n",
    "\n",
    "            h = buffer.next()\n",
    "\n",
    "            # print(h.shape)\n",
    "\n",
    "            loss_dict, loss, acts, _ = self.forward(h)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            l2_loss = loss_dict[\"L_reconstruction\"]\n",
    "            l1_loss = loss_dict[\"L_sparsity\"]\n",
    "            \n",
    "            assert(l1_loss.shape[0] == h.shape[0] and l2_loss.shape[0] == h.shape[0])\n",
    "            l2_loss = l2_loss.mean()\n",
    "            l1_loss = (self.cfg.l1_coefficient * l1_loss).mean()\n",
    "            l0 = (acts > 0).float().sum(-1).mean()\n",
    "            frac_active = (acts.abs() > 1e-8).float().mean(0)\n",
    "            frac_active_list.append(frac_active)\n",
    "            \n",
    "            log_dict = {\"losses/loss\": loss, \"losses/l1_loss\":l1_loss, \"losses/l2_loss\": l2_loss, 'metrics/frac_active':frac_active.mean().item(), \"metrics/l0\": l0}\n",
    "\n",
    "            if not self.cfg.tied_weights:\n",
    "                    self.W_dec.data = self.W_dec_normalized\n",
    "\n",
    "            wandb.log(log_dict)\n",
    "            \n",
    "            if step % self.cfg.log_freq == 0 or (step + 1 == steps):\n",
    "                progress_bar.set_postfix(\n",
    "                    lr=step_lr,\n",
    "                    frac_active=frac_active.mean().item(),\n",
    "                    **{k: v.mean(0).sum().item() for k, v in loss_dict.items()},  # type: ignore\n",
    "                )\n",
    "                # data_log[\"W_enc\"].append(self.W_enc.detach().cpu().clone())\n",
    "                # data_log[\"W_dec\"].append(self.W_dec.detach().cpu().clone())\n",
    "                # data_log[\"frac_active\"].append(frac_active.detach().cpu().clone())\n",
    "                # data_log[\"steps\"].append(step)\n",
    "            # return dat\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def resample_advanced(\n",
    "        self,\n",
    "        frac_active_in_window: Float[Tensor, \"window d_sae\"],\n",
    "        resample_scale: float,\n",
    "        batch_size: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Resamples latents that have been dead for 'dead_feature_window' steps, according to `frac_active`.\n",
    "\n",
    "        Resampling method is:\n",
    "            - Compute the L2 reconstruction loss produced from the hidden state vectors `h`\n",
    "            - Randomly choose values of `h` with probability proportional to their reconstruction loss\n",
    "            - Set new values of W_dec and W_enc to be these (centered and normalized) vectors, at each dead neuron\n",
    "            - Set b_enc to be zero, at each dead neuron\n",
    "\n",
    "        Returns colors and titles (useful for creating the animation: resampled neurons appear in red).\n",
    "        \"\"\"\n",
    "        h = buffer.next()\n",
    "        l2_loss = self.forward(h)[0][\"L_reconstruction\"]\n",
    "\n",
    "        # Find the dead latents in this instance. If all latents are alive, continue\n",
    "        is_dead = (frac_active_in_window < 1e-8).all(dim=0)\n",
    "        dead_latents = torch.nonzero(is_dead).squeeze(-1)\n",
    "        n_dead = dead_latents.numel()\n",
    "        if n_dead == 0:\n",
    "            return  # If we have no dead features, then we don't need to resampl\n",
    "\n",
    "        # Compute L2 loss for each element in the batch\n",
    "        l2_loss_instance = l2_loss  # [batch_size]\n",
    "        if l2_loss_instance.max() < 1e-6:\n",
    "            return  # If we have zero reconstruction loss, we don't need to resample\n",
    "\n",
    "        # Draw `d_sae` samples from [0, 1, ..., batch_size-1], with probabilities proportional to l2_loss\n",
    "        distn = Categorical(probs=l2_loss_instance.pow(2) / l2_loss_instance.pow(2).sum())\n",
    "        replacement_indices = distn.sample((n_dead,))  # type: ignore\n",
    "\n",
    "        # Index into the batch of hidden activations to get our replacement values\n",
    "        replacement_values = (h - self.b_dec)[replacement_indices]  # [n_dead d_in]\n",
    "        replacement_values_normalized = replacement_values / (\n",
    "            replacement_values.norm(dim=-1, keepdim=True) + self.cfg.weight_normalize_eps\n",
    "        )\n",
    "\n",
    "        # Get the norm of alive neurons (or 1.0 if there are no alive neurons)\n",
    "        W_enc_norm_alive_mean = (\n",
    "            self.W_enc[:, ~is_dead].norm(dim=0).mean().item()\n",
    "            if (~is_dead).any()\n",
    "            else 1.0\n",
    "        )\n",
    "\n",
    "        # Lastly, set the new weights & biases (W_dec is normalized, W_enc needs specific scaling, b_enc is zero)\n",
    "        self.W_dec.data[dead_latents, :] = replacement_values_normalized\n",
    "        self.W_enc.data[:, dead_latents] = (\n",
    "            replacement_values_normalized.T * W_enc_norm_alive_mean * resample_scale\n",
    "        )\n",
    "        self.b_enc.data[dead_latents] = 0.0\n",
    "\n",
    "    \"\"\"\n",
    "        Forward\n",
    "        Optimize\n",
    "        resample\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ff1b29-99bb-47a8-ac91-ba7d57bbf0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "sae = SAE(cfg, model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "944fc9eb-f9fb-4e05-9578-65851fa9bd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33misistickz\u001b[0m (\u001b[33mself_research_\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/israel/SAEs/wandb/run-20241006_134117-73zqyghr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/self_research_/Autoencoders/runs/73zqyghr' target=\"_blank\">L8_24576_L1-8e-05_Lr-0.0004</a></strong> to <a href='https://wandb.ai/self_research_/Autoencoders' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/self_research_/Autoencoders' target=\"_blank\">https://wandb.ai/self_research_/Autoencoders</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/self_research_/Autoencoders/runs/73zqyghr' target=\"_blank\">https://wandb.ai/self_research_/Autoencoders/runs/73zqyghr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [39:06<00:00, 12.79it/s, L_reconstruction=8.84e+6, L_sparsity=4.79e+5, frac_active=0.645, lr=0.0004]  \n"
     ]
    }
   ],
   "source": [
    "sae.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f56ee203-861e-4cb4-9d01-6445a4dc4bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as version 1\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = Path(\"trained_saes\")\n",
    "\n",
    "\n",
    "def save():\n",
    "    version = 1\n",
    "    name = f\"L{sae.cfg.layer}_{sae.cfg.d_sae}_L1-{sae.cfg.l1_coefficient}_Lr-{sae.cfg.lr}_V{version}\"\n",
    "    torch.save(sae.state_dict(), SAVE_DIR/(name+\".pt\"))\n",
    "    with open(SAVE_DIR/(name+\"_cfg.json\"), \"w\") as f:\n",
    "        json.dump(asdict(cfg), f)\n",
    "    print(\"Saved as version\", version)\n",
    "\n",
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0decb6d-e97d-4436-b6ff-23944e473636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                   Tokens which most activate this feature                    </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Sequence                                                      </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ \\n# WITHOUT WARRANTIES<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> OR</span> CONDITIONS OF                       │ 23.76      │\n",
       "│  freely, subject to the<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> following</span> restrictions:\\n             │ 22.44      │\n",
       "│  version 3 of the License<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">,</span> or                                 │ 22.30      │\n",
       "│  the Free Software Foundation,<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> either</span> version 3 of the        │ 22.00      │\n",
       "│ as-is', without<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> any</span> express or implied                        │ 21.74      │\n",
       "│  ANY KIND, either<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> express</span> or implied.\\n                       │ 21.60      │\n",
       "│ # freely, subject to<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> the</span> following restrictions:              │ 21.21      │\n",
       "│ ET HISTORY: Pike<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> Ch</span>ocofest began                              │ 21.01      │\n",
       "│ \\n# freely, subject<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> to</span> the following restrictions:            │ 20.96      │\n",
       "│  OF ANY KIND,<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> either</span> express or implied.                      │ 20.89      │\n",
       "│ IS,\\n# WITHOUT<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> WARRANT</span>IES OR CONDIT                           │ 20.84      │\n",
       "│ -is', without any<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> express</span> or implied\\n                        │ 20.21      │\n",
       "│ ## the Free Software Foundation<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">,</span> either version 3 of          │ 20.18      │\n",
       "│ # This notebook covers only<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> very</span> basic classification models. │ 19.89      │\n",
       "│  'as-is',<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> without</span> any express or implied                      │ 19.82      │\n",
       "│  ##\\n## it under<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> the</span> terms of the GNU                         │ 19.81      │\n",
       "│  the terms of the GNU<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> General</span> Public License as published     │ 19.76      │\n",
       "│ ## (at your option<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">)</span> any later version.                        │ 19.66      │\n",
       "│  having to exchange for a<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> larger</span> size. I found                │ 19.42      │\n",
       "│  service industry is not easy<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> and</span> it takes have Good          │ 19.39      │\n",
       "└───────────────────────────────────────────────────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                   Tokens which most activate this feature                    \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mSequence                                                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ \\n# WITHOUT WARRANTIES\u001b[1;4;38;5;208m OR\u001b[0m CONDITIONS OF                       │ 23.76      │\n",
       "│  freely, subject to the\u001b[1;4;38;5;208m following\u001b[0m restrictions:\\n             │ 22.44      │\n",
       "│  version 3 of the License\u001b[1;4;38;5;208m,\u001b[0m or                                 │ 22.30      │\n",
       "│  the Free Software Foundation,\u001b[1;4;38;5;208m either\u001b[0m version 3 of the        │ 22.00      │\n",
       "│ as-is', without\u001b[1;4;38;5;208m any\u001b[0m express or implied                        │ 21.74      │\n",
       "│  ANY KIND, either\u001b[1;4;38;5;208m express\u001b[0m or implied.\\n                       │ 21.60      │\n",
       "│ # freely, subject to\u001b[1;4;38;5;208m the\u001b[0m following restrictions:              │ 21.21      │\n",
       "│ ET HISTORY: Pike\u001b[1;4;38;5;208m Ch\u001b[0mocofest began                              │ 21.01      │\n",
       "│ \\n# freely, subject\u001b[1;4;38;5;208m to\u001b[0m the following restrictions:            │ 20.96      │\n",
       "│  OF ANY KIND,\u001b[1;4;38;5;208m either\u001b[0m express or implied.                      │ 20.89      │\n",
       "│ IS,\\n# WITHOUT\u001b[1;4;38;5;208m WARRANT\u001b[0mIES OR CONDIT                           │ 20.84      │\n",
       "│ -is', without any\u001b[1;4;38;5;208m express\u001b[0m or implied\\n                        │ 20.21      │\n",
       "│ ## the Free Software Foundation\u001b[1;4;38;5;208m,\u001b[0m either version 3 of          │ 20.18      │\n",
       "│ # This notebook covers only\u001b[1;4;38;5;208m very\u001b[0m basic classification models. │ 19.89      │\n",
       "│  'as-is',\u001b[1;4;38;5;208m without\u001b[0m any express or implied                      │ 19.82      │\n",
       "│  ##\\n## it under\u001b[1;4;38;5;208m the\u001b[0m terms of the GNU                         │ 19.81      │\n",
       "│  the terms of the GNU\u001b[1;4;38;5;208m General\u001b[0m Public License as published     │ 19.76      │\n",
       "│ ## (at your option\u001b[1;4;38;5;208m)\u001b[0m any later version.                        │ 19.66      │\n",
       "│  having to exchange for a\u001b[1;4;38;5;208m larger\u001b[0m size. I found                │ 19.42      │\n",
       "│  service industry is not easy\u001b[1;4;38;5;208m and\u001b[0m it takes have Good          │ 19.39      │\n",
       "└───────────────────────────────────────────────────────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def highest_activating_tokens(\n",
    "    tokens: Int[Tensor, \"batch seq\"],\n",
    "    model: HookedTransformer,\n",
    "    autoencoder: SAE,\n",
    "    feature_idx: int,\n",
    "    autoencoder_B: bool = False,\n",
    "    k: int = 20,\n",
    ") -> Tuple[Int[Tensor, \"k 2\"], Float[Tensor, \"k\"]]:\n",
    "    '''\n",
    "    Returns the indices & values for the highest-activating tokens in the given batch of data.\n",
    "    '''\n",
    "    batch_size, seq_len = tokens.shape\n",
    "    # instance_idx = 1 if autoencoder_B else 0/\n",
    "\n",
    "    # Get the post activations from the clean run\n",
    "    cache = model.run_with_cache(tokens, names_filter=[\"blocks.8.hook_resid_post\"])[1]\n",
    "    post = cache[\"blocks.8.hook_resid_post\"]\n",
    "    post_reshaped = einops.rearrange(post, \"batch seq d_model -> (batch seq) d_model\")\n",
    "\n",
    "    # Compute activations (not from a fwd pass, but explicitly, by taking only the feature we want)\n",
    "    # This code is copied from the first part of the 'forward' method of the AutoEncoder class\n",
    "    h_cent = post_reshaped - autoencoder.b_dec\n",
    "    acts = einops.einsum(\n",
    "        h_cent, autoencoder.W_enc[:, feature_idx],\n",
    "        \"batch_size n_input_ae, n_input_ae -> batch_size\"\n",
    "    )\n",
    "\n",
    "    # Get the top k largest activations\n",
    "    top_acts_values, top_acts_indices = acts.topk(k)\n",
    "\n",
    "    # Convert the indices into (batch, seq) indices\n",
    "    top_acts_batch = top_acts_indices // seq_len\n",
    "    top_acts_seq = top_acts_indices % seq_len\n",
    "\n",
    "    return torch.stack([top_acts_batch, top_acts_seq], dim=-1), top_acts_values\n",
    "\n",
    "\n",
    "def display_top_sequences(top_acts_indices, top_acts_values, tokens):\n",
    "    table = Table(\"Sequence\", \"Activation\", title=\"Tokens which most activate this feature\")\n",
    "    for (batch_idx, seq_idx), value in zip(top_acts_indices, top_acts_values):\n",
    "        # Get the sequence as a string (with some padding on either side of our sequence)\n",
    "        seq = \"\"\n",
    "        for i in range(max(seq_idx-5, 0), min(seq_idx+5, all_tokens.shape[1])):\n",
    "            new_str_token = model.to_single_str_token(tokens[batch_idx, i].item()).replace(\"\\n\", \"\\\\n\")\n",
    "            # Highlight the token with the high activation\n",
    "            if i == seq_idx: new_str_token = f\"[b u dark_orange]{new_str_token}[/]\"\n",
    "            seq += new_str_token\n",
    "        # Print the sequence, and the activation value\n",
    "        table.add_row(seq, f'{value:.2f}')\n",
    "    rprint(table)\n",
    "\n",
    "tokens = all_tokens[:200]\n",
    "top_acts_indices, top_acts_values = highest_activating_tokens(tokens, model, sae, feature_idx=11111, autoencoder_B=False)\n",
    "display_top_sequences(top_acts_indices, top_acts_values, tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MechInterp",
   "language": "python",
   "name": "mechintep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
